#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
run_review.py
- PR diffì˜ ì„¹ì…˜ì„ "í•¨ìˆ˜/ë©”ì„œë“œ ê²½ê³„" ê¸°ì¤€ìœ¼ë¡œ êµ¬ì„±í•˜ê³ ,
  ë„ˆë¬´ í¬ë©´ ì˜¤ë²„ë© ë¶„í• í•˜ë˜, ì„ ì–¸ë¶€(ìƒë‹¨ ë³€ìˆ˜/í´ë˜ìŠ¤ í•„ë“œ/ì„í¬íŠ¸)ê¹Œì§€ í¬í•¨í•´
  LLMì´ ì¤‘ê°„ì—ì„œ ëŠê¸°ì§€ ì•Šë„ë¡ ë§¥ë½ì„ ì œê³µí•©ë‹ˆë‹¤.
- OpenAI Chat Completionsë¡œ í˜¸ì¶œ â†’ JSON ì‘ë‹µ â†’ PR ìš”ì•½/ì¸ë¼ì¸ ì½”ë©˜íŠ¸ ì—…ë¡œë“œ.
"""

import os, re, json, subprocess, requests, pathlib, sys
from collections import defaultdict

# ===== í™˜ê²½ ë³€ìˆ˜ =====
REPO = os.getenv("GITHUB_REPOSITORY")
EVENT = json.load(open(os.getenv("GITHUB_EVENT_PATH")))
PR_NUM = EVENT["pull_request"]["number"]
HEAD_SHA = EVENT["pull_request"]["head"]["sha"]

BASE_BRANCH = os.getenv("BASE_BRANCH", "main")
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# ì„¹ì…˜/ë§¥ë½ íŒŒë¼ë¯¸í„°(í•„ìš”ì‹œ ì›Œí¬í”Œë¡œìš°ì—ì„œ ENVë¡œ ì¡°ì •)
MAX_LINES_PER_SECTION = int(os.getenv("MAX_LINES_PER_SECTION", "220"))  # í•œ ì„¹ì…˜ ìµœëŒ€ ë¼ì¸ ìˆ˜
OVERLAP_LINES         = int(os.getenv("OVERLAP_LINES", "10"))           # ë¶„í•  ì‹œ ì˜¤ë²„ë©
FUNC_CTX_BEFORE       = int(os.getenv("FUNC_CTX_BEFORE", "16"))         # í•¨ìˆ˜ ì‹œì‘ ìœ„ ì»¨í…ìŠ¤íŠ¸(ì„ ì–¸/í•„ë“œ í¬í•¨)
NUM_CTX_LINES         = int(os.getenv("NUM_CTX_LINES", "6"))            # ì„¹ì…˜ ì£¼ë³€ ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸
MAX_PAYLOAD_CHARS     = int(os.getenv("MAX_PAYLOAD_CHARS", "180000"))   # í•œ ë²ˆ í˜¸ì¶œ ìµœëŒ€ í˜ì´ë¡œë“œ
PER_FILE_CALL         = os.getenv("PER_FILE_CALL", "true").lower() == "true"  # íŒŒì¼ ë‹¨ìœ„ë¡œ ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ

# ===== ìœ í‹¸ =====

def summarize_sections(hunks_by_file):
    """LLM í˜¸ì¶œ ì—†ì´ ì„¹ì…˜ë§Œ ê³„ì‚°í•´ì„œ ì‚¬ëŒì´ ë³´ê¸° ì¢‹ê²Œ ìš”ì•½"""
    out = []
    for path, hunks in hunks_by_file.items():
        secs = sections_for_file(path, hunks)
        if not secs:
            continue
        out.append(f"## {path}")
        for (p, s, e), text in secs:
            # ì„¹ì…˜ í¬ê¸°/í”„ë¦¬ë·°(ì²« ì¤„, ë ì¤„)
            lines = text.splitlines()
            preview_begin = lines[0] if lines else ""
            preview_end   = lines[-1] if lines else ""
            out.append(f"- section: {s}~{e}  (len={len(text)})")
            out.append(f"  - begin: `{preview_begin[:120]}`")
            out.append(f"  - end  : `{preview_end[:120]}`")
    return "\n".join(out) if out else "ì„¹ì…˜ ì—†ìŒ(ë³€ê²½ì´ ì—†ê±°ë‚˜ ctags ë¯¸ì„¤ì¹˜)"

def debug_sections_and_exit(hunks_by_file):
    body = "### ğŸ” Section Debug\n" + summarize_sections(hunks_by_file)
    post_summary(body)   # PR ì½”ë©˜íŠ¸ë¡œ ë‚¨ê¹€


def sh(*cmd: str) -> str:
    return subprocess.check_output(list(cmd), text=True).strip()

def get_diff_unified0() -> str:
    base = sh("git", "merge-base", f"origin/{BASE_BRANCH}", "HEAD")
    return subprocess.check_output(["git","diff",f"{base}...HEAD","--unified=0"], text=True)

def parse_hunks(diff: str):
    """
    @@ -a,b +c,d @@ ë¥¼ íŒŒì‹±í•´ ìƒˆ ì½”ë“œ ì˜ì—­(+c,d)ì˜ ì‹œì‘/ê¸¸ì´ë¥¼ ì¶”ì¶œ.
    return: [(path, start, end), ...]
    """
    sections = []
    cur_file = None
    for line in diff.splitlines():
        if line.startswith("+++ b/"):
            cur_file = line[6:].strip()
        elif line.startswith("+++ /dev/null"):
            cur_file = None
        m = re.match(r"@@ -\d+(?:,\d+)? \+(\d+)(?:,(\d+))? @@", line)
        if cur_file and m:
            start = int(m.group(1))
            length = int(m.group(2) or "1")
            sections.append((cur_file, start, start + length - 1))
    return sections

def load_lines(path: str):
    text = pathlib.Path(path).read_text(encoding="utf-8", errors="ignore")
    return text.splitlines()

# ===== ctags ê¸°ë°˜ ì‹¬ë³¼ ìˆ˜ì§‘ =====
def have_ctags() -> bool:
    try:
        subprocess.check_output(["ctags", "--version"])
        return True
    except Exception:
        return False

def ctags_symbols(path: str):
    """
    universal-ctags ê°€ì •. `ctags -n -x <file>` ì¶œë ¥ íŒŒì‹±.
    ë¼ì¸ ì˜ˆ: name  kind  line  file
    """
    try:
        out = subprocess.check_output(["ctags","-n","-x",path], text=True)
    except Exception:
        return []
    symbols = []
    for line in out.splitlines():
        # ëŠìŠ¨í•œ íŒŒì‹±: name kind line file...
        m = re.match(r"(\S+)\s+(\S+)\s+(\d+)\s+(.*)$", line)
        if not m: 
            continue
        name, kind, lno, _ = m.groups()
        try:
            lno = int(lno)
        except ValueError:
            continue
        symbols.append({"name": name, "kind": kind.lower(), "line": lno})
    symbols.sort(key=lambda x: x["line"])
    return symbols

def function_ranges_with_ctags(path: str, total_lines: int):
    """
    í•¨ìˆ˜/ë©”ì„œë“œ ì‹œì‘ ë¼ì¸ ëª©ë¡ â†’ (start, end) ë²”ìœ„ ì¶”ì •
    ë‹¤ìŒ ì‹¬ë³¼ ì‹œì‘ - 1 ê¹Œì§€ë¥¼ ëìœ¼ë¡œ ë³¸ë‹¤(ì™„ë²½í•˜ì§„ ì•Šì§€ë§Œ ì‹¤ìš©ì ).
    """
    if not have_ctags(): 
        return []
    syms = ctags_symbols(path)
    fn_starts = [s["line"] for s in syms if s["kind"] in ("function","method")]
    if not fn_starts:
        return []
    fn_starts.sort()
    ranges = []
    for i, s in enumerate(fn_starts):
        e = (fn_starts[i+1]-1) if i+1 < len(fn_starts) else total_lines
        ranges.append((s, e))
    return ranges

def class_ranges_with_ctags(path: str, total_lines: int):
    if not have_ctags():
        return []
    syms = ctags_symbols(path)
    cls_starts = [s["line"] for s in syms if s["kind"] in ("class","struct")]
    if not cls_starts:
        return []
    cls_starts.sort()
    ranges = []
    for i, s in enumerate(cls_starts):
        e = (cls_starts[i+1]-1) if i+1 < len(cls_starts) else total_lines
        ranges.append((s, e))
    return ranges

def enclosing_class_start(path: str, line: int, total_lines: int):
    """í•´ë‹¹ ë¼ì¸ì´ ì†í•œ í´ë˜ìŠ¤ ì‹œì‘ ë¼ì¸(ì—†ìœ¼ë©´ None)"""
    for cs, ce in class_ranges_with_ctags(path, total_lines):
        if cs <= line <= ce:
            return cs
    return None

# ===== ë¶„í• /í™•ì¥ ë¡œì§ =====
def split_with_overlap(start: int, end: int, max_lines=MAX_LINES_PER_SECTION, overlap=OVERLAP_LINES):
    """[start,end] ë²”ìœ„ë¥¼ ì˜¤ë²„ë©ìœ¼ë¡œ ë¶„í• """
    n = end - start + 1
    if n <= max_lines:
        yield (start, end); return
    cur = start
    while cur <= end:
        ps = cur
        pe = min(end, cur + max_lines - 1)
        yield (ps, pe)
        if pe == end: break
        cur = max(pe - overlap + 1, pe + 1)

def expand_range_for_decls(path: str,
                           func_start: int,
                           func_end: int,
                           total_lines: int,
                           func_ranges: list):
    """
    - í•¨ìˆ˜ ì‹œì‘ ìœ„ë¡œ FUNC_CTX_BEFORE ì¤„ í™•ì¥ (ì„ ì–¸/ì„í¬íŠ¸ ë§¥ë½)
    - í´ë˜ìŠ¤ ë‚´ë¶€ë©´ í´ë˜ìŠ¤ ì‹œì‘ ì´ì „ìœ¼ë¡œëŠ” í™•ì¥ ê¸ˆì§€
    - **ì´ì „ í•¨ìˆ˜ì˜ ë(fe) ì´ì „ìœ¼ë¡œëŠ” í™•ì¥ ê¸ˆì§€**  â† í•µì‹¬
    """
    cls_start = enclosing_class_start(path, func_start, total_lines)

    # ê¸°ë³¸ ì•µì»¤: í•¨ìˆ˜ ì‹œì‘ ìœ„ë¡œ Nì¤„
    anchor = func_start - FUNC_CTX_BEFORE

    # í´ë˜ìŠ¤ ë²”ìœ„ ì¡´ì¤‘
    if cls_start:
        anchor = max(cls_start, anchor)

    # ë°”ë¡œ ì´ì „ í•¨ìˆ˜ì˜ ë ì°¾ê¸° (ìˆìœ¼ë©´ ê·¸ ë‹¤ìŒ ì¤„ë¶€í„° ì‹œì‘)
    prev_func_end = 0
    for fs, fe in func_ranges:
        if fe < func_start and fe > prev_func_end:
            prev_func_end = fe

    start = max(1, anchor, prev_func_end + 1)
    end = func_end
    return (start, end)

def sections_for_file(path: str, hunks_for_file: list):
    """
    - ctags í•¨ìˆ˜ ë²”ìœ„ê°€ ìˆìœ¼ë©´ â†’ hunkì™€ ê²¹ì¹˜ëŠ” í•¨ìˆ˜ ë‹¨ìœ„ë¡œ ì„¹ì…˜ ìƒì„±
    - ì—†ìœ¼ë©´ â†’ hunk ë²”ìœ„ë¥¼ ìœˆë„ìš° ë¶„í• 
    - ìƒì„± í›„ **í¬í•¨/ì¤‘ë³µ ì„¹ì…˜ ì œê±°**ë¡œ ê²¹ì¹¨ ìµœì†Œí™”
    """
    if not os.path.exists(path):
        return []

    lines = load_lines(path)
    total = len(lines)
    func_ranges = function_ranges_with_ctags(path, total)

    merged = merge_intervals(hunks_for_file)
    sections = []

    if func_ranges:
        for hs, he in merged:
            for fs, fe in func_ranges:
                if intervals_overlap(hs, he, fs, fe):
                    # ì´ì „ í•¨ìˆ˜ ê²½ê³„ ê³ ë ¤í•˜ì—¬ í™•ì¥
                    es, ee = expand_range_for_decls(path, fs, fe, total, func_ranges)
                    # í•„ìš” ì‹œ ë¶„í• 
                    for ps, pe in split_with_overlap(es, ee):
                        sec = numbered_section(path, ps, pe, lines, ctx=NUM_CTX_LINES)
                        if sec:
                            sections.append((path, ps, pe, sec))
    else:
        for hs, he in merged:
            for ps, pe in split_with_overlap(hs, he):
                sec = numbered_section(path, ps, pe, lines, ctx=NUM_CTX_LINES)
                if sec:
                    sections.append((path, ps, pe, sec))

    # -------- í¬í•¨/ì¤‘ë³µ ì œê±° --------
    # ê°™ì€ íŒŒì¼ ë‚´ì—ì„œ start/end ê¸°ì¤€ ì •ë ¬
    sections.sort(key=lambda x: (x[0], x[1], x[2]))

    pruned = []
    for p, s, e, t in sections:
        # 1) ì´ë¯¸ ë™ì¼ êµ¬ê°„ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if any(pp == p and ss == s and ee == e for (pp, ss, ee, _) in pruned):
            continue
        # 2) ë” í° êµ¬ê°„ì´ ì´ë¯¸ í¬í•¨í•˜ê³  ìˆìœ¼ë©´ ìŠ¤í‚µ (s~eê°€ ê¸°ì¡´ì— í¬í•¨ë¨)
        if any(pp == p and ss <= s and e <= ee for (pp, ss, ee, _) in pruned):
            continue
        # 3) ë°˜ëŒ€ë¡œ, ë‚´ê°€ ê¸°ì¡´ êµ¬ê°„ì„ ì™„ì „íˆ í¬í•¨í•˜ë©´ ê¸°ì¡´ ê²ƒì„ ì œê±°í•˜ê³  ë‚˜ë¥¼ ë„£ê¸°
        pruned = [(pp, ss, ee, tt) for (pp, ss, ee, tt) in pruned
                  if not (pp == p and s <= ss and ee <= e)]
        pruned.append((p, s, e, t))

    # ë°˜í™˜ í˜•íƒœ ë§ì¶”ê¸°: [((path,start,end), section_text), ...]
    uniq = {}
    for p, s, e, t in pruned:
        uniq[(p, s, e)] = t
    return list(uniq.items())

def intervals_overlap(a1, a2, b1, b2):
    return not (a2 < b1 or b2 < a1)

def merge_intervals(spans):
    """[(s,e), ...] â†’ ê²¹ì¹˜ëŠ” êµ¬ê°„ ë³‘í•©"""
    if not spans:
        return []
    spans = sorted(spans)
    merged = [spans[0]]
    for s, e in spans[1:]:
        ls, le = merged[-1]
        if s <= le + 1:
            merged[-1] = (ls, max(le, e))
        else:
            merged.append((s, e))
    return merged

def numbered_section(path: str, start: int, end: int, lines=None, ctx=NUM_CTX_LINES):
    """ë¼ì¸ ë²ˆí˜¸ê°€ í¬í•¨ëœ ì„¹ì…˜(ì»¨í…ìŠ¤íŠ¸ Â±ctx í¬í•¨)"""
    try:
        if lines is None:
            lines = load_lines(path)
    except Exception:
        return None
    s = max(1, start - ctx)
    e = min(len(lines), end + ctx)
    body = "\n".join(f"{i+1}: {lines[i]}" for i in range(s-1, e))
    return f'<SECTION file="{path}" start={start} end={end}>\n{body}\n</SECTION>'

# ===== LLM í˜¸ì¶œ/ë¦¬í¬íŒ… =====
def build_messages(payload_text: str):
    base_dir = pathlib.Path(__file__).resolve().parent  # â† ì¶”ê°€
    sys_prompt = (base_dir / "prompt.md").read_text(encoding="utf-8")  # â† ê²½ë¡œ ìˆ˜ì •
    return [
        {"role": "system", "content": sys_prompt},
        {"role": "user",   "content": payload_text if payload_text.strip() else "ë³€ê²½ ì„¹ì…˜ ì—†ìŒ"}
    ]

def call_openai(messages):
    model = MODEL
    use_responses = model.startswith("o4")  # o4, o4-mini

    if use_responses:
        url = "https://api.openai.com/v1/responses"

        # messages(list[{"role","content"}]) -> Responses ì…ë ¥ í¬ë§·
        def to_responses_input(ms):
            return [
                {
                    "role": m["role"],
                    "content": [
                        {"type": "input_text", "text": m["content"]}  # <-- í•µì‹¬: input_text
                    ],
                } for m in ms
            ]

        payload = {
            "model": model,
            "input": to_responses_input(messages),
            "max_output_tokens": 1200,   # temperature ë„£ì§€ ì•Šê¸°
        }
    else:
        url = "https://api.openai.com/v1/chat/completions"
        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.2,
            "max_tokens": 1200,
        }

    r = requests.post(url,
        headers={"Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}",
                 "Content-Type": "application/json"},
        data=json.dumps(payload),
        timeout=180
    )
    if r.status_code != 200:
        print("OpenAI error:", r.status_code, r.text)
        r.raise_for_status()

    data = r.json()

    # ì•ˆì „ íŒŒì‹± (Responses)
    def extract_text_from_responses(d):
        if d.get("output_text"):
            return d["output_text"]
        for msg in d.get("output", []) or []:
            for part in msg.get("content", []) or []:
                if part.get("type") in ("output_text", "summary_text") and "text" in part:
                    return part["text"]
        # fallback (ì˜ˆì™¸ì  ë¼ìš°íŒ…)
        if d.get("choices"):
            return d["choices"][0]["message"]["content"]
        return json.dumps(d, ensure_ascii=False)

    content = extract_text_from_responses(data) if use_responses else data["choices"][0]["message"]["content"]

    s, e = content.find("{"), content.rfind("}")
    try:
        return json.loads(content[s:e+1])
    except Exception:
        return {"diagnosis": [], "issues": [], "overall_summary": content}

def post_summary(body: str):
    url = f"https://api.github.com/repos/{REPO}/issues/{PR_NUM}/comments"
    requests.post(url,
        headers={"Authorization": f"Bearer {os.environ['GITHUB_TOKEN']}",
                 "Accept":"application/vnd.github+json"},
        json={"body": body}
    ).raise_for_status()

def post_inline(issues: list):
    """ë‹¨ì¼/ë‹¤ì¤‘ ë¼ì¸ ì¸ë¼ì¸ ì½”ë©˜íŠ¸ + suggestion ì§€ì›"""
    url = f"https://api.github.com/repos/{REPO}/pulls/{PR_NUM}/comments"
    headers = {"Authorization": f"Bearer {os.environ['GITHUB_TOKEN']}",
               "Accept":"application/vnd.github+json"}
    for it in issues[:60]:
        path = it.get("file")
        line = it.get("line")
        sline = it.get("start_line")
        eline = it.get("end_line", line)
        if not path or (not line and not sline):
            continue
        title = f"**{it.get('type','Issue')} ({it.get('severity','minor')})**"
        reason = it.get("reason","")
        suggestion = it.get("suggestion","")
        body = f"{title}\n{reason}\n\n{suggestion}"
        payload = {"body": body, "path": path, "side":"RIGHT", "commit_id": HEAD_SHA}
        if sline:
            payload["start_line"] = int(sline)
            payload["line"] = int(eline)
        else:
            payload["line"] = int(line)
        requests.post(url, headers=headers, json=payload).raise_for_status()

def summarize_diag(diag: list):
    if not diag: 
        return "ë°œê²¬ëœ ìš”ì•½ ì—†ìŒ"
    out = []
    for d in diag:
        t = d.get("type","-"); c = d.get("count",0); s = d.get("summary","")
        out.append(f"- **{t}**: {c}ê±´ â€” {s}")
    return "\n".join(out)

# ===== ë©”ì¸: íŒŒì¼/í•¨ìˆ˜ ê²½ê³„ ê¸°ë°˜ ì„¹ì…˜ ìƒì„± â†’ í˜¸ì¶œ â†’ ì—…ë¡œë“œ =====
def build_payload_all_at_once(hunks_by_file):
    """ëª¨ë“  íŒŒì¼ ì„¹ì…˜ì„ í•œ ë²ˆì— í•©ì³ì„œ ì „ì†¡ (ì†Œí˜• PRìš©)"""
    blocks = []
    for path, hunks in hunks_by_file.items():
        for (_, s, e), sec in sections_for_file(path, hunks):
            blocks.append(sec)
    return "\n\n".join(blocks)[:MAX_PAYLOAD_CHARS]

def per_file_calls(hunks_by_file):
    all_issues, all_diag = [], []
    summaries = []

    for path, hunks in hunks_by_file.items():
        secs = sections_for_file(path, hunks)
        if not secs:
            continue
        batch, size = [], 0
        for (_, s, e), text in secs:
            if size + len(text) > MAX_PAYLOAD_CHARS and batch:
                res = call_openai(build_messages("\n\n".join(batch)))
                all_diag += res.get("diagnosis", [])
                all_issues += res.get("issues", [])
                if res.get("overall_summary"): summaries.append(f"### {path}\n{res['overall_summary']}")
                batch, size = [text], len(text)
            else:
                batch.append(text); size += len(text)
        if batch:
            res = call_openai(build_messages("\n\n".join(batch)))
            all_diag += res.get("diagnosis", [])
            all_issues += res.get("issues", [])
            if res.get("overall_summary"): summaries.append(f"### {path}\n{res['overall_summary']}")

    if summaries:
        try:
            post_summary("#### Raw summaries (debug)\n" + "\n\n".join(summaries)[:6000])
        except Exception:
            pass
    return all_diag, all_issues

def main():
    diff = get_diff_unified0()
    hunks = parse_hunks(diff)
    if not hunks:
        post_summary("ë³€ê²½ ì„¹ì…˜ì´ ì—†ì–´ ë¦¬ë·°ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")
        return

    # íŒŒì¼ë³„ë¡œ ëª¨ìœ¼ê¸°
    hunks_by_file = defaultdict(list)
    for path, st, en in hunks:
        hunks_by_file[path].append((st, en))
        
  #ì„¹ì…˜ ë””ë²„ê·¸ ëª¨ë“œ
    if os.getenv("SECTIONS_DEBUG", "0") == "1":
        debug_sections_and_exit(hunks_by_file)
        return

    if PER_FILE_CALL:
        diag, issues = per_file_calls(hunks_by_file)
        post_summary("### ğŸ¤– LLM Code Review ìš”ì•½\n" + summarize_diag(diag))
        post_inline(issues)
    else:
        payload = build_payload_all_at_once(hunks_by_file)
        result = call_openai(build_messages(payload))
        post_summary("### ğŸ¤– LLM Code Review ìš”ì•½\n" + summarize_diag(result.get("diagnosis", [])) + 
                     "\n\n---\n" + result.get("overall_summary",""))
        post_inline(result.get("issues", []))

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        try:
            post_summary(f"ë¦¬ë·° ì‘ì—… ì¤‘ ì˜ˆì™¸ ë°œìƒ: `{e}`")
        finally:
            raise
