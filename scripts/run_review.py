#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
run_review.py
- PR diffë¥¼ í•¨ìˆ˜/ë©”ì„œë“œ ê²½ê³„ ê¸°ì¤€ ì„¹ì…˜ìœ¼ë¡œ ë‚˜ëˆ  LLMì— ì „ë‹¬
- ê²°ê³¼ë¥¼ ì¸ë¼ì¸ ì½”ë©˜íŠ¸(ë³€ê²½ ë¼ì¸ë§Œ) + PR ìƒë‹¨ ìš”ì•½(ì—…ì„œíŠ¸)ë¡œ ê²Œì‹œ
- ìš”ì•½ í˜•ì‹: Diagnosis(ì¹´í…Œê³ ë¦¬ ì§‘ê³„/ìš”ì•½)ë§Œ í‘œì‹œ
"""

import os, re, json, subprocess, requests, pathlib, sys
from collections import defaultdict, Counter

# ===== GitHub / ëª¨ë¸ ì„¤ì • =====
REPO = os.getenv("GITHUB_REPOSITORY")
EVENT = json.load(open(os.getenv("GITHUB_EVENT_PATH")))
PR_NUM = EVENT["pull_request"]["number"]
HEAD_SHA = EVENT["pull_request"]["head"]["sha"]

BASE_BRANCH = os.getenv("BASE_BRANCH", "main")
MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# ===== ì„¹ì…˜/ë§¥ë½ íŒŒë¼ë¯¸í„° =====
MAX_LINES_PER_SECTION = int(os.getenv("MAX_LINES_PER_SECTION", "220"))
OVERLAP_LINES         = int(os.getenv("OVERLAP_LINES", "10"))
FUNC_CTX_BEFORE       = int(os.getenv("FUNC_CTX_BEFORE", "16"))
NUM_CTX_LINES         = int(os.getenv("NUM_CTX_LINES", "6"))
MAX_PAYLOAD_CHARS     = int(os.getenv("MAX_PAYLOAD_CHARS", "180000"))
PER_FILE_CALL         = os.getenv("PER_FILE_CALL", "true").lower() == "true"

# ===== ì¹´í…Œê³ ë¦¬ ì§‘ê³„ =====
ORDER = ["Precondition", "Runtime", "Optimization", "Security"]
SUMMARIES = {
    "Precondition": "ì½”ë“œ ì‹¤í–‰ ì „ ì…ë ¥ê°’Â·ìƒíƒœÂ·ë²”ìœ„Â·ë™ì‹œì„± ì¡°ê±´ ë“±ì„ ì‚¬ì „ì— ê²€ì¦í•˜ëŠ” ë¶€ë¶„",
    "Runtime": "ì‹¤í–‰ ì¤‘ NPE, ì¸ë±ìŠ¤ ë²”ìœ„ ì˜¤ë¥˜, 0ìœ¼ë¡œ ë‚˜ëˆ”, ìì› ëˆ„ìˆ˜, ë°ë“œë½Â·ë ˆì´ìŠ¤ ë“± ì•ˆì •ì„± ê´€ë ¨ ë¬¸ì œ",
    "Optimization": "ë¶ˆí•„ìš”í•œ ì—°ì‚°Â·I/OÂ·ë™ê¸°í™”, ë°ì´í„° ë³µì‚¬, N+1 ì¿¼ë¦¬, ë¶€ì ì ˆí•œ ë™ê¸°â†”ë¹„ë™ê¸° ë³€í™˜ ë“± ì„±ëŠ¥ ë¹„íš¨ìœ¨",
    "Security": "ì‹œí¬ë¦¿Â·ë¯¼ê°ì •ë³´ ë…¸ì¶œ, ê²½ë¡œ ì¡°ì‘, SQL ì¸ì ì…˜, ì•ˆì „í•˜ì§€ ì•Šì€ ì§ë ¬í™”/ëª¨ë“ˆ ì‚¬ìš© ë“± ë³´ì•ˆ ì·¨ì•½ì ",
}

def classify(issue_type: str, reason: str) -> str:
    """ê°œë³„ ì´ìŠˆë¥¼ 4ê°œ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë§¤í•‘(í•œ/ì˜ í‚¤ì›Œë“œ í¬í•¨)."""
    t = (issue_type or "").lower()
    r = (reason or "").lower()
    txt = t + " " + r

    # Security
    if any(k in txt for k in [
        "secret","token","credential","path traversal","sql","injection","pii","serialize","pickle","jwt","xss","csrf",
        "ë¹„ë°€","ì‹œí¬ë¦¿","í† í°","ê²½ë¡œ ì¡°ì‘","ì¸ì ì…˜","ë¯¼ê°ì •ë³´","ì·¨ì•½","ê¶Œí•œ ìƒìŠ¹"
    ]):
        return "Security"

    # Optimization
    if any(k in txt for k in [
        "n+1","unnecessary io","copy","deepcopy","blocking","busy loop","complexity","async","synchronous","inefficient",
        "ë¶ˆí•„ìš”","ë¹„íš¨ìœ¨","ê³¼ë„í•œ ë³µì‚¬","ë™ê¸°í™” ë‚¨ìš©","ì„±ëŠ¥","ëŠë¦¼"
    ]):
        return "Optimization"

    # Runtime
    if any(k in txt for k in [
        "npe","nullpointer","attributeerror","index","out_of_bounds","keyerror","zero","divide","/0","leak","deadlock","race","resource",
        "0ìœ¼ë¡œ","ì œë¡œ","ë¶„ëª¨ 0","ë‚˜ëˆ—ì…ˆ ì˜¤ë¥˜","ì¸ë±ìŠ¤ ë²”ìœ„","ìì› ëˆ„ìˆ˜","ë°ë“œë½","ë ˆì´ìŠ¤"
    ]):
        return "Runtime"

    # Precondition
    if any(k in txt for k in [
        "precondition","validate","validation","null check","range","bounds","thread-safety","input check","guard",
        "mutable default","ê°€ë³€ ê¸°ë³¸ê°’","ê¸°ë³¸ ì¸ìˆ˜","ì…ë ¥ ê²€ì¦","ë²”ìœ„ ê²€ì¦","ë™ì‹œì„± ì „ì œ","ì‚¬ì „ì¡°ê±´"
    ]):
        return "Precondition"

    return "Precondition"

def build_aggregated_diagnosis(issues: list) -> list:
    """ì´ìŠˆ ë°°ì—´ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ í•©ì‚°í•˜ì—¬ 4ê°œ í•­ëª©ë§Œ ë°˜í™˜."""
    counter = Counter()
    for it in issues or []:
        cat = classify(it.get("type",""), it.get("reason",""))
        counter[cat] += 1
    return [{"type": cat, "count": int(counter.get(cat, 0)), "summary": SUMMARIES[cat]} for cat in ORDER]

# --- suggestion helpers ---
SUGG_RX = re.compile(r"(?s)```suggestion\s*\n(.*?)\n```")

def _extract_suggestion_block(text: str) -> str:
    if not text:
        return ""
    m = SUGG_RX.search(text)
    return m.group(0).strip() if m else ""

def _is_multiline_sugg(block: str) -> bool:
    if not block:
        return False
    inner = SUGG_RX.search(block).group(1)
    return inner.count("\n") >= 1

# ===== ê³µí†µ ìœ í‹¸ =====
def sh(*cmd: str) -> str:
    return subprocess.check_output(list(cmd), text=True).strip()

def get_diff_unified0() -> str:
    base = sh("git", "merge-base", f"origin/{BASE_BRANCH}", "HEAD")
    return subprocess.check_output(["git","diff",f"{base}...HEAD","--unified=0"], text=True)

def parse_hunks(diff: str):
    """@@ -a,b +c,d @@ ë¸”ë¡ì„ íŒŒì‹± â†’ (path, start, end) ë¦¬ìŠ¤íŠ¸"""
    sections = []
    cur_file = None
    for line in diff.splitlines():
        if line.startswith("+++ b/"):
            cur_file = line[6:].strip()
        elif line.startswith("+++ /dev/null"):
            cur_file = None
        m = re.match(r"@@ -\d+(?:,\d+)? \+(\d+)(?:,(\d+))? @@", line)
        if cur_file and m:
            start = int(m.group(1))
            length = int(m.group(2) or "1")
            sections.append((cur_file, start, start + length - 1))
    return sections

def load_lines(path: str):
    text = pathlib.Path(path).read_text(encoding="utf-8", errors="ignore")
    return text.splitlines()

# ===== ctags ê¸°ë°˜ ì‹¬ë³¼ =====
def have_ctags() -> bool:
    try:
        subprocess.check_output(["ctags", "--version"])
        return True
    except Exception:
        return False

def ctags_symbols(path: str):
    try:
        out = subprocess.check_output(["ctags","-n","-x",path], text=True)
    except Exception:
        return []
    symbols = []
    for line in out.splitlines():
        m = re.match(r"(\S+)\s+(\S+)\s+(\d+)\s+(.*)$", line)
        if not m:
            continue
        name, kind, lno, _ = m.groups()
        try:
            lno = int(lno)
        except ValueError:
            continue
        symbols.append({"name": name, "kind": kind.lower(), "line": lno})
    symbols.sort(key=lambda x: x["line"])
    return symbols

def function_ranges_with_ctags(path: str, total_lines: int):
    if not have_ctags():
        return []
    syms = ctags_symbols(path)
    fn_starts = [s["line"] for s in syms if s["kind"] in ("function","method")]
    if not fn_starts:
        return []
    fn_starts.sort()
    ranges = []
    for i, s in enumerate(fn_starts):
        e = (fn_starts[i+1]-1) if i+1 < len(fn_starts) else total_lines
        ranges.append((s, e))
    return ranges

def class_ranges_with_ctags(path: str, total_lines: int):
    if not have_ctags():
        return []
    syms = ctags_symbols(path)
    cls_starts = [s["line"] for s in syms if s["kind"] in ("class","struct")]
    if not cls_starts:
        return []
    cls_starts.sort()
    ranges = []
    for i, s in enumerate(cls_starts):
        e = (cls_starts[i+1]-1) if i+1 < len(cls_starts) else total_lines
        ranges.append((s, e))
    return ranges

def enclosing_class_start(path: str, line: int, total_lines: int):
    for cs, ce in class_ranges_with_ctags(path, total_lines):
        if cs <= line <= ce:
            return cs
    return None

# ===== ë¶„í• /í™•ì¥ =====
def split_with_overlap(start: int, end: int, max_lines=MAX_LINES_PER_SECTION, overlap=OVERLAP_LINES):
    n = end - start + 1
    if n <= max_lines:
        yield (start, end); return
    cur = start
    while cur <= end:
        ps = cur
        pe = min(end, cur + max_lines - 1)
        yield (ps, pe)
        if pe == end: break
        cur = max(pe - overlap + 1, pe + 1)

def expand_range_for_decls(path: str, func_start: int, func_end: int, total_lines: int, func_ranges: list):
    """
    - í•¨ìˆ˜ ì‹œì‘ ìœ„ë¡œ FUNC_CTX_BEFOREì¤„ í™•ì¥
    - í´ë˜ìŠ¤ ë‚´ë¶€ë©´ í´ë˜ìŠ¤ ì‹œì‘ ì´ì „ìœ¼ë¡œ í™•ì¥ ê¸ˆì§€
    - ë°”ë¡œ ì´ì „ í•¨ìˆ˜ì˜ ë ì´ì „ìœ¼ë¡œ í™•ì¥ ê¸ˆì§€
    """
    cls_start = enclosing_class_start(path, func_start, total_lines)
    anchor = func_start - FUNC_CTX_BEFORE
    if cls_start:
        anchor = max(cls_start, anchor)
    prev_func_end = 0
    for fs, fe in func_ranges:
        if fe < func_start and fe > prev_func_end:
            prev_func_end = fe
    start = max(1, anchor, prev_func_end + 1)
    end = func_end
    return (start, end)

def intervals_overlap(a1, a2, b1, b2):
    return not (a2 < b1 or b2 < a1)

def merge_intervals(spans):
    if not spans:
        return []
    spans = sorted(spans)
    merged = [spans[0]]
    for s, e in spans[1:]:
        ls, le = merged[-1]
        if s <= le + 1:
            merged[-1] = (ls, max(le, e))
        else:
            merged.append((s, e))
    return merged

def numbered_section(path: str, start: int, end: int, lines=None, ctx=NUM_CTX_LINES):
    try:
        if lines is None:
            lines = load_lines(path)
    except Exception:
        return None
    s = max(1, start - ctx)
    e = min(len(lines), end + ctx)
    body = "\n".join(f"{i+1}: {lines[i]}" for i in range(s-1, e))
    return f'<SECTION file="{path}" start={start} end={end}>\n{body}\n</SECTION>'

def sections_for_file(path: str, hunks_for_file: list):
    if not os.path.exists(path):
        return []
    lines = load_lines(path)
    total = len(lines)
    func_ranges = function_ranges_with_ctags(path, total)

    merged = merge_intervals(hunks_for_file)
    sections = []

    if func_ranges:
        for hs, he in merged:
            for fs, fe in func_ranges:
                if intervals_overlap(hs, he, fs, fe):
                    es, ee = expand_range_for_decls(path, fs, fe, total, func_ranges)
                    for ps, pe in split_with_overlap(es, ee):
                        sec = numbered_section(path, ps, pe, lines, ctx=NUM_CTX_LINES)
                        if sec:
                            sections.append((path, ps, pe, sec))
    else:
        for hs, he in merged:
            for ps, pe in split_with_overlap(hs, he):
                sec = numbered_section(path, ps, pe, lines, ctx=NUM_CTX_LINES)
                if sec:
                    sections.append((path, ps, pe, sec))

    # í¬í•¨/ì¤‘ë³µ ì œê±°
    sections.sort(key=lambda x: (x[0], x[1], x[2]))
    pruned = []
    for p, s, e, t in sections:
        if any(pp == p and ss == s and ee == e for (pp, ss, ee, _) in pruned):
            continue
        if any(pp == p and ss <= s and e <= ee for (pp, ss, ee, _) in pruned):
            continue
        pruned = [(pp, ss, ee, tt) for (pp, ss, ee, tt) in pruned
                  if not (pp == p and s <= ss and ee <= e)]
        pruned.append((p, s, e, t))

    uniq = {}
    for p, s, e, t in pruned:
        uniq[(p, s, e)] = t
    return list(uniq.items())

# ===== LLM í˜¸ì¶œ =====
def build_messages(payload_text: str):
    base_dir = pathlib.Path(__file__).resolve().parent
    sys_prompt = (base_dir / "prompt.md").read_text(encoding="utf-8")
    return [
        {"role": "system", "content": sys_prompt},
        {"role": "user",   "content": payload_text if payload_text.strip() else "ë³€ê²½ ì„¹ì…˜ ì—†ìŒ"}
    ]

def call_openai(messages):
    model = MODEL.strip()
    use_responses = model.startswith("o4")  # o4, o4-mini ë“±ë§Œ responses ì‚¬ìš©

    if use_responses:
        url = "https://api.openai.com/v1/responses"

        def to_responses_input(ms):
            # ğŸ”´ ëª¨ë“  role(system/user/developer ë“±)ì— ëŒ€í•´ input_text ì‚¬ìš©
            return [
                {
                    "role": m["role"],
                    "content": [{"type": "input_text", "text": m["content"]}],
                }
                for m in ms
            ]

        payload = {
            "model": model,
            "input": to_responses_input(messages),
            "max_output_tokens": 1200,
        }
    else:
        url = "https://api.openai.com/v1/chat/completions"
        payload = {
            "model": model,
            "messages": messages,
            "temperature": 0.2,
            "max_tokens": 1200,
        }

    headers = {
        "Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}",
        "Content-Type": "application/json",
    }

    r = requests.post(url, headers=headers, json=payload, timeout=180)
    try:
        r.raise_for_status()
    except requests.HTTPError as e:
        raise requests.HTTPError(f"{e} â€” body={r.text[:2000]}")

    data = r.json()

    def extract_text_from_responses(d):
        if d.get("output_text"):
            return d["output_text"]
        try:
            for msg in d.get("output", []) or []:
                for part in msg.get("content", []) or []:
                    if part.get("text"):
                        return part["text"]
        except Exception:
            pass
        if d.get("choices"):
            return d["choices"][0]["message"]["content"]
        return json.dumps(d, ensure_ascii=False)

    content = extract_text_from_responses(data) if use_responses \
              else data["choices"][0]["message"]["content"]

    s, e = content.find("{"), content.rfind("}")
    try:
        parsed = json.loads(content[s:e+1])
    except Exception:
        parsed = {"diagnosis": [], "issues": [], "overall_summary": content}
    return parsed, content

# ===== GitHub í¬ìŠ¤íŒ… =====
def post_summary(body: str):
    url = f"https://api.github.com/repos/{REPO}/issues/{PR_NUM}/comments"
    requests.post(url,
        headers={"Authorization": f"Bearer {os.environ['GITHUB_TOKEN']}"},
        json={"body": body}
    ).raise_for_status()

SUMMARY_TAG = "<!-- LLM-CODE-REVIEW-SUMMARY -->"

def upsert_summary_comment(body: str):
    """PRì— ìš”ì•½ ì½”ë©˜íŠ¸ë¥¼ í•˜ë‚˜ë§Œ ìœ ì§€(ì—…ì„œíŠ¸). PATCHëŠ” /issues/comments/:id ê²½ë¡œ."""
    base = f"https://api.github.com/repos/{REPO}"
    headers = {"Authorization": f"Bearer {os.environ['GITHUB_TOKEN']}"}

    list_url = f"{base}/issues/{PR_NUM}/comments"
    r = requests.get(list_url, headers=headers); r.raise_for_status()
    comments = r.json() or []

    target = next((c for c in reversed(comments)
                   if isinstance(c.get("body"), str) and SUMMARY_TAG in c["body"]), None)

    body_with_tag = body + f"\n\n{SUMMARY_TAG}"

    if target:
        edit_url = f"{base}/issues/comments/{target['id']}"
        pr = requests.patch(edit_url, headers=headers, json={"body": body_with_tag})
        if pr.status_code == 404:
            requests.post(list_url, headers=headers, json={"body": body_with_tag}).raise_for_status()
        else:
            pr.raise_for_status()
    else:
        requests.post(list_url, headers=headers, json={"body": body_with_tag}).raise_for_status()

def post_review_summary(body: str):
    upsert_summary_comment(body)

def _overlaps_hunks(path: str, it: dict, hunks_by_file: dict) -> bool:
    """ì´ìŠˆê°€ diff ë²”ìœ„ì— í¬í•¨ë˜ëŠ”ì§€ ì²´í¬(ì¸ë¼ì¸ ê°€ëŠ¥ ì—¬ë¶€)."""
    hunks = hunks_by_file.get(path, [])
    line = it.get("line")
    sline = it.get("start_line")
    eline = it.get("end_line") or line

    if line:
        line = int(line)
        return any(s <= line <= e for (s, e) in hunks)
    if sline and eline:
        sline = int(sline); eline = int(eline)
        return any(not (eline < s or e < sline) for (s, e) in hunks)
    return False

def post_inline(issues: list, hunks_by_file: dict):
    """ë³€ê²½ ë¼ì¸(=diff í¬í•¨)ë§Œ ì¸ë¼ì¸ ì½”ë©˜íŠ¸ë¡œ ê²Œì‹œ."""
    url = f"https://api.github.com/repos/{REPO}/pulls/{PR_NUM}/comments"
    headers = {"Authorization": f"Bearer {os.environ['GITHUB_TOKEN']}"}

    # ì •ë ¬: severity â†’ file â†’ line
    sev_rank = {"critical":0, "major":1, "minor":2}
    issues = sorted(issues, key=lambda it: (
        sev_rank.get((it.get("severity") or "minor").lower(), 9),
        it.get("file",""),
        it.get("line") or it.get("start_line") or 10**9
    ))

    for it in issues[:200]:
        path = it.get("file")
        if not path or not _overlaps_hunks(path, it, hunks_by_file):
            continue

        title = f"**{it.get('type','Issue')} ({it.get('severity','minor')})**"
        reason = (it.get("reason","") or "").strip()

        # suggestion ì •ê·œí™”
        sugg_block = _extract_suggestion_block(it.get("suggestion",""))
        has_range = it.get("start_line") is not None and it.get("end_line") is not None
        has_line  = it.get("line") is not None

        # ë‹¤ì¤‘ ë¼ì¸ ì œì•ˆì¸ë° ë²”ìœ„ê°€ ì—†ìœ¼ë©´ í˜¼ì„  ë°©ì§€: ì½”ë©˜íŠ¸ë§Œ ë‚¨ê¹€
        if sugg_block and _is_multiline_sugg(sugg_block) and not has_range:
            body = f"{title}\n{reason}"
        else:
            body = f"{title}\n{reason}\n\n{sugg_block}" if sugg_block else f"{title}\n{reason}"

        payload = {"body": body, "path": path, "side":"RIGHT", "commit_id": HEAD_SHA}
        if has_range:
            payload["start_line"] = int(it["start_line"])
            payload["line"] = int(it.get("end_line", it["start_line"]))
        elif has_line:
            payload["line"] = int(it["line"])
        else:
            continue  # ìœ„ì¹˜ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì—…ë¡œë“œ ë¶ˆê°€

        requests.post(url, headers=headers, json=payload)
        # ì‹¤íŒ¨í•´ë„ ì¡°ìš©íˆ ì§„í–‰

# ===== ìš”ì•½(Diagnosisë§Œ) =====
def build_summary_markdown(diag: list, inline_issues: list, out_of_diff: list) -> str:
    """Diagnosisë§Œ ì¶œë ¥."""
    sev_emoji = {"critical":"ğŸ›‘", "major":"âš ï¸", "minor":"â„¹ï¸", "info":"ğŸ“"}
    all_issues = (inline_issues or []) + (out_of_diff or [])

    # ì „ì²´ ì‹¬ê°ë„ ë°°ì§€
    by_sev = defaultdict(int)
    for it in all_issues:
        by_sev[(it.get("severity") or "minor").lower()] += 1
    badge = " ".join(f"{sev_emoji.get(k,'â€¢')} {k.capitalize()}: **{by_sev.get(k,0)}**"
                     for k in ("critical","major","minor","info"))

    rows = []
    for d in (diag or []):
        cat = d.get("type","-")
        summary = d.get("summary","")
        count = d.get("count",0)
        rows.append(f"- **{cat}** â€” {count}ê±´\n  Â· {summary}")
    diagnosis_md = "\n".join(rows) if rows else "_ìš”ì•½ ì—†ìŒ_"

    return (
        "## ğŸ§­ LLM Code Review Summary\n\n"
        f"- ì´ ì´ìŠˆ: **{len(all_issues)}**  |  {badge}\n\n"
        "### Diagnosis\n"
        f"{diagnosis_md}"
    )

# ===== ì„¹ì…˜ ì¹´ë“œ(ì°¸ê³ ) =====
def extract_plain_code_from_section(section_text: str) -> str:
    lines = section_text.splitlines()
    try:
        start_idx = next(i for i, ln in enumerate(lines) if ln.startswith("<SECTION "))
        end_idx   = len(lines) - 1 - next(i for i, ln in enumerate(reversed(lines)) if ln.strip() == "</SECTION>")
    except StopIteration:
        body = lines
    else:
        body = lines[start_idx+1:end_idx]
    plain = []
    for ln in body:
        m = re.match(r"^\s*\d+:\s?(.*)$", ln)
        plain.append(m.group(1) if m else ln)
    return "\n".join(plain)

def format_section_card_md(path: str, s: int, e: int, section_text: str, parsed: dict) -> str:
    code_block = extract_plain_code_from_section(section_text)
    issues = []
    for it in parsed.get("issues", []) or []:
        if (it.get("file") == path) and (
            (it.get("line") and s <= int(it["line"]) <= e) or
            (it.get("start_line") and it.get("end_line") and
             not (int(it["end_line"]) < s or e < int(it["start_line"])) )
        ):
            issues.append(it)
    if issues:
        issues_md = "\n".join(
            f"- **{it.get('type','Issue')}** ({it.get('severity','minor')}) "
            f"@ L{it.get('start_line', it.get('line'))}"
            f"{'-L'+str(it['end_line']) if it.get('start_line') else ''} â€” {it.get('reason','')}"
            for it in issues
        )
        first_sugg = next((it.get("suggestion") for it in issues if it.get("suggestion")), "")
    else:
        issues_md = "_No issues detected in this section._"; first_sugg = ""
    md = []
    md.append(f"### `{path}` {s}â€“{e}\n")
    md.append("```python"); md.append(code_block); md.append("```")
    md.append("\n**Findings**"); md.append(issues_md)
    if first_sugg: md.append("\n**Suggested change**\n" + first_sugg)
    return "\n".join(md)

def per_file_calls(hunks_by_file):
    all_issues = []
    section_cards = []
    for path, hunks in hunks_by_file.items():
        secs = sections_for_file(path, hunks)
        if not secs: continue
        for (_, s, e), section_text in secs:
            parsed, _raw = call_openai(build_messages(section_text))
            section_cards.append(format_section_card_md(path, s, e, section_text, parsed))
            all_issues += parsed.get("issues", [])
    if section_cards:
        try:
            post_review_summary("## ğŸ“¦ LLM Code Review (by section)\n" + "\n\n---\n\n".join(section_cards))
        except Exception:
            pass
    return [], all_issues

# ===== ë©”ì¸ =====
def build_payload_all_at_once(hunks_by_file):
    blocks = []
    for path, hunks in hunks_by_file.items():
        for (_, s, e), sec in sections_for_file(path, hunks):
            blocks.append(sec)
    return "\n\n".join(blocks)[:MAX_PAYLOAD_CHARS]

def main():
    diff = get_diff_unified0()
    hunks = parse_hunks(diff)
    if not hunks:
        upsert_summary_comment("## ğŸ§­ LLM Code Review Summary\në³€ê²½ ì„¹ì…˜ì´ ì—†ì–´ ë¦¬ë·°ë¥¼ ìƒëµí•©ë‹ˆë‹¤.\n\n" + SUMMARY_TAG)
        return

    # íŒŒì¼ë³„ë¡œ ëª¨ìœ¼ê¸°
    hunks_by_file = defaultdict(list)
    for path, st, en in hunks:
        hunks_by_file[path].append((st, en))

    # ì„¹ì…˜ ë””ë²„ê·¸
    if os.getenv("SECTIONS_DEBUG", "0") == "1":
        body = "### ğŸ” Section Debug\n"
        for p, h in hunks_by_file.items(): body += f"- {p}: {h}\n"
        post_summary(body); return

    # í˜¸ì¶œ ë°©ì‹
    if PER_FILE_CALL:
        _diag_ignored, issues = per_file_calls(hunks_by_file)
    else:
        payload = build_payload_all_at_once(hunks_by_file)
        parsed, _raw = call_openai(build_messages(payload))
        issues = parsed.get("issues", [])

    # ì¸ë¼ì¸ ê°€ëŠ¥ vs ë¶ˆê°€ ë¶„ë¦¬
    inline_candidates, out_of_diff = [], []
    for it in issues:
        path = it.get("file")
        (inline_candidates if path and _overlaps_hunks(path, it, hunks_by_file) else out_of_diff).append(it)

    # ì¸ë¼ì¸ ì—…ë¡œë“œ
    post_inline(inline_candidates, hunks_by_file)

    # í™”ë©´ì— ë³´ì´ëŠ” ì „ì²´ ì´ìŠˆ ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„
    all_issues = inline_candidates + out_of_diff
    aggregated_diag = build_aggregated_diagnosis(all_issues)

    # ìƒë‹¨ ìš”ì•½ ì—…ì„œíŠ¸
    summary_md = build_summary_markdown(aggregated_diag, inline_candidates, out_of_diff)
    upsert_summary_comment(summary_md)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        try:
            post_summary(f"ë¦¬ë·° ì‘ì—… ì¤‘ ì˜ˆì™¸ ë°œìƒ: `{e}`")
        finally:
            raise
